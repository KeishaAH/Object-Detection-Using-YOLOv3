# -*- coding: utf-8 -*-
"""dl_final_code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ToFGxjXRz7F27imnEdmO7sFXVzMvyVYF
"""

# Commented out IPython magic to ensure Python compatibility.
# Clone Darknet warehouse
!git clone https://github.com/AlexeyAB/darknet

# Go to the Darknet directory
# %cd darknet

# Install the OpenCV library
!apt-get install -y libopencv-dev

# Compile Darknet
!make

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/darknet

!ls

!sed -i 's/OPENCV=0/OPENCV=1/' Makefile

!make clean
!make

# Download YOLOv3's pre-training weights
!wget https://pjreddie.com/media/files/yolov3.weights

# Run YOLOv3 for inference, input an image
!./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg

import cv2
from google.colab.patches import cv2_imshow

# Read result image
img = cv2.imread('predictions.jpg')

# Display image
cv2_imshow(img)

!pip install pycocotools

import os

# Create a directory to store the data set
os.makedirs('/content/coco/val2017', exist_ok=True)
os.makedirs('/content/coco/annotations', exist_ok=True)

# Download COCO 2017 validation set image and annotation file
!wget -P /content/coco/val2017/ http://images.cocodataset.org/zips/val2017.zip
!wget -P /content/coco/annotations/ http://images.cocodataset.org/annotations/annotations_trainval2017.zip

# Extract validation set images and annotation files
!unzip /content/coco/val2017/val2017.zip -d /content/coco/val2017/
!unzip /content/coco/annotations/annotations_trainval2017.zip -d /content/coco/annotations/

"""# Randomly choose 10 images from COCO validation dataset"""

import random
import os

# Set the validation set image path
val_images_dir = '/content/coco/val2017/val2017'

# Gets all image paths of the validation set
val_images = [os.path.join(val_images_dir, img) for img in os.listdir(val_images_dir) if img.endswith('.jpg')]

# 10 images are randomly selected
sample_images = random.sample(val_images, 10)

# Save the selected 10 image paths to a text file
sample_file = '/content/coco/annotations/sample_val2017.txt'
with open(sample_file, 'w') as f:
    for img in sample_images:
        f.write(img + '\n')

print(f"The {len(sample_images)} image has been selected and the path has been saved to {sample_file}")

import os

# Create a directory to save test results
os.makedirs('/content/coco/predictions', exist_ok=True)

import os
import json
import re

# Create output directory
output_dir = '/content/coco/predictions'
os.makedirs(output_dir, exist_ok=True)

# Delete the previous prediction file
for filename in os.listdir(output_dir):
    file_path = os.path.join(output_dir, filename)
    if os.path.isfile(file_path):
        os.remove(file_path)

# Read 10 image paths
with open('/content/coco/annotations/sample_val2017.txt', 'r') as f:
    image_paths = f.readlines()

# Removes line breaks from the path
image_paths = [path.strip() for path in image_paths]

# Map the class name to the COCO ID
class_name_to_coco_id = {
    'person': 1,
    'bicycle': 2,
    'car': 3,
    'motorbike': 4,
    'airplane': 5,
    'bus': 6,
    'train': 7,
    'truck': 8,
    'boat': 9,
    'traffic light': 10,
    'fire hydrant': 11,
    'stop sign': 12,
    'parking meter': 13,
    'bench': 14,
    'bird': 15,
    'cat': 16,
    'dog': 17,
    'horse': 18,
    'sheep': 19,
    'cow': 20,
    'elephant': 21,
    'bear': 22,
    'zebra': 23,
    'giraffe': 24,
    'bag': 25,
    'backpack': 26,
    'umbrella': 27,
    'handbag': 28,
    'tie': 29,
    'suitcase': 30,
    'frisbee': 31,
    'skis': 32,
    'snowboard': 33,
    'sports ball': 34,
    'kite': 35,
    'baseball bat': 36,
    'baseball glove': 37,
    'skateboard': 38,
    'surfboard': 39,
    'tennis racket': 40,
    'bottle': 41,
    'wine glass': 42,
    'cup': 43,
    'fork': 44,
    'knife': 45,
    'spoon': 46,
    'bowl': 47,
    'banana': 48,
    'apple': 49,
    'sandwich': 50,
    'orange': 51,
    'broccoli': 52,
    'carrot': 53,
    'hot dog': 54,
    'pizza': 55,
    'donut': 56,
    'cake': 57,
    'chair': 58,
    'couch': 59,
    'potted plant': 60,
    'bed': 61,
    'dining table': 62,
    'toilet': 63,
    'tv': 64,
    'laptop': 65,
    'mouse': 66,
    'remote': 67,
    'keyboard': 68,
    'cell phone': 69,
    'microwave': 70,
    'oven': 71,
    'toaster': 72,
    'sink': 73,
    'refrigerator': 74,
    'book': 75,
    'clock': 76,
    'vase': 77,
    'scissors': 78,
    'teddy bear': 79,
    'hair drier': 80
}

# Perform the YOLOv3 check and save the result
for img_path in image_paths:
    img_name = img_path.split('/')[-1]

    # The path to the output text file
    yolo_output_file = f'/content/coco/predictions/{img_name}_predictions.txt'

    # Use YOLOv3 to reason and save the results
    os.system(f'./darknet detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights {img_path} -ext_output > {yolo_output_file}')

    # Read YOLOv3 inference results and convert to COCO format
    predictions = []
    with open(yolo_output_file, 'r') as f:
        lines = f.readlines()

        for line in lines:
            # Matches the detection output line
            match = re.match(r'(\w+):\s+(\d+)%\s+\(left_x:\s+(\d+)\s+top_y:\s+(\d+)\s+width:\s+(\d+)\s+height:\s+(\d+)\)', line.strip())
            if match:
                class_name = match.group(1)  # Class name
                confidence = float(match.group(2)) / 100  # Confidence converted to decimal form
                left_x = int(match.group(3))
                top_y = int(match.group(4))
                width = int(match.group(5))
                height = int(match.group(6))

                # Map the class name to the COCO class ID
                category_id = class_name_to_coco_id.get(class_name)

                if category_id:
                    # Convert YOLO format to COCO format
                    prediction = {
                        'image_id': img_name.split('.')[0],  # Use the image file name as the image_id
                        'category_id': category_id,  # Use the COCO class ID
                        'bbox': [left_x, top_y, width, height],
                        'score': confidence  # Confidence degree
                    }
                    predictions.append(prediction)

    # Save as a JSON file in COCO format
    output_json_file = f'/content/coco/predictions/{img_name}_predictions.json'
    with open(output_json_file, 'w') as out_file:
        json.dump(predictions, out_file, indent=4)

    print(f'Processed {img_name} and saved predictions to {output_json_file}')

print(predictions[:5])  # Print the first five predictions to check

"""## The logic of this method is wrong, but we decided to keep it and analyze the cause of the error.The correct answer is at the end."""

from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, f1_score, confusion_matrix
import json
import os
import numpy as np

# Data path settings
gt_file = '/content/coco/annotations/annotations/instances_val2017.json'  # Ground truth path
predictions_folder = '/content/coco/predictions'  # Predictions path

# Load Ground Truth annotations
with open(gt_file, 'r') as f:
    coco_gt = json.load(f)

# Parse Ground Truth
ground_truths = {}
for annotation in coco_gt['annotations']:
    image_id = annotation['image_id']
    bbox = annotation['bbox']
    # Convert bbox format to [x1, y1, x2, y2]
    x1, y1, w, h = bbox
    x2, y2 = x1 + w, y1 + h
    if image_id not in ground_truths:
        ground_truths[image_id] = []
    ground_truths[image_id].append([x1, y1, x2, y2])

# Load prediction results
predictions = {}
for json_file in os.listdir(predictions_folder):
    if json_file.endswith('.json'):
        # Extract image_id
        image_id_str = json_file.split('_')[0]
        image_id = int(image_id_str.replace('.jpg', ''))
        with open(os.path.join(predictions_folder, json_file), 'r') as f:
            pred_data = json.load(f)
            for prediction in pred_data:
                bbox = prediction['bbox']
                score = prediction['score']
                # Convert bbox format to [x1, y1, x2, y2]
                x1, y1, w, h = bbox
                x2, y2 = x1 + w, y1 + h
                if image_id not in predictions:
                    predictions[image_id] = []
                predictions[image_id].append([x1, y1, x2, y2, score])

# IoU calculation function
def compute_iou(box1, box2):
    """Calculate the IoU of two bounding boxes"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    inter_area = max(0, x2 - x1) * max(0, y2 - y1)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - inter_area
    return inter_area / union_area if union_area > 0 else 0

# Compute mAP and output related metrics
def calculate_map(ground_truths, predictions, iou_thresholds=[0.5, 0.75]):
    aps = []
    for iou_thresh in iou_thresholds:
        y_true_all = []
        y_score_all = []

        for image_id in ground_truths:
            gt_boxes = ground_truths.get(image_id, [])
            pred_boxes = predictions.get(image_id, [])

            matched_gt = set()
            y_true = []
            y_score = []

            for pred_box in pred_boxes:
                max_iou = 0
                matched_idx = -1
                for j, gt_box in enumerate(gt_boxes):
                    if j in matched_gt:
                        continue
                    iou = compute_iou(pred_box[:4], gt_box)
                    if iou > max_iou and iou >= iou_thresh:
                        max_iou = iou
                        matched_idx = j

                if matched_idx >= 0:
                    matched_gt.add(matched_idx)
                    y_true.append(1)  # Positive
                else:
                    y_true.append(0)  # Negative
                y_score.append(pred_box[4])  # Score

            for j in range(len(gt_boxes)):
                if j not in matched_gt:
                    y_true.append(1)  # Positive
                    y_score.append(0)  # Score is 0 (not detected)

            y_true_all.extend(y_true)
            y_score_all.extend(y_score)

        # Precision-Recall curve and AP
        if y_true_all and y_score_all:
            precision, recall, _ = precision_recall_curve(y_true_all, y_score_all)
            ap = auc(recall, precision)
            aps.append(ap)

            # Precision, Recall, F1 Score, and Confusion Matrix
            binary_predictions = [1 if score >= 0.5 else 0 for score in y_score_all]
            precision_value = precision_score(y_true_all, binary_predictions)
            recall_value = recall_score(y_true_all, binary_predictions)
            f1_value = f1_score(y_true_all, binary_predictions)
            cm = confusion_matrix(y_true_all, binary_predictions)

            print(f"IoU Threshold: {iou_thresh}")
            print(f"Precision: {precision_value:.4f}, Recall: {recall_value:.4f}, F1 Score: {f1_value:.4f}")
            print(f"Confusion Matrix:\n{cm}\n")
        else:
            aps.append(0.0)

    return np.mean(aps)

# Compute and output mAP and related metrics
map_value = calculate_map(ground_truths, predictions, iou_thresholds=[0.5, 0.75])
print(f"mAP: {map_value:.4f}")

"""# Correct method for COCO"""

import json
import os
import numpy as np

def compute_iou(box1, box2):
    """Calculate IoU (Intersection over Union) of two bounding boxes."""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[0] + box1[2], box2[0] + box2[2])
    y2 = min(box1[1] + box1[3], box2[1] + box2[3])

    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = box1[2] * box1[3]
    area2 = box2[2] * box2[3]
    union = area1 + area2 - intersection

    return intersection / union if union > 0 else 0

def compute_ap(precisions, recalls):
    """Calculate the AP (Average Precision) value."""
    recalls = [0] + recalls + [1]
    precisions = [0] + precisions + [0]

    for i in range(len(precisions) - 1, 0, -1):
        precisions[i - 1] = max(precisions[i - 1], precisions[i])

    indices = [i for i in range(1, len(recalls)) if recalls[i] != recalls[i - 1]]
    ap = sum((recalls[i] - recalls[i - 1]) * precisions[i] for i in indices)
    return ap

def compute_dataset_metrics(predictions, ground_truths, categories, iou_threshold=0.5):
    """Compute overall dataset mAP, average Precision, Recall, and generate a total confusion matrix."""
    all_precisions = []
    all_recalls = []
    aps = []

    # Initialize the total confusion matrix
    total_confusion_matrix = np.zeros((2, 2), dtype=int)  # [TP, FP; FN, TN]
    confusion_matrices = {}

    for category_id in categories:
        print(f"\n### Calculating AP for Category {category_id} ###")

        # Filter predictions and ground truths by category
        category_preds = [pred for pred in predictions if pred.get('category_id') == category_id]
        category_gts = [gt for gt in ground_truths if gt.get('category_id') == category_id]

        # Sort predictions by score in descending order
        category_preds = sorted(category_preds, key=lambda x: x.get('score', 0), reverse=True)

        # Mark whether ground truths are used (to avoid modifying the original list)
        matched_gts = [dict(gt, used=False) for gt in category_gts]

        tp, fp = 0, 0
        tps, fps = [], []

        for pred in category_preds:
            max_iou = 0
            matched_gt = None

            for gt in matched_gts:
                if gt['used']:
                    continue
                iou = compute_iou(pred['bbox'], gt['bbox'])
                if iou > max_iou:
                    max_iou = iou
                    matched_gt = gt

            if max_iou >= iou_threshold:
                tp += 1
                total_confusion_matrix[0, 0] += 1  # TP
                if matched_gt is not None:
                    matched_gt['used'] = True
            else:
                fp += 1
                total_confusion_matrix[0, 1] += 1  # FP

            # Accumulate TP and FP
            tps.append(tp)
            fps.append(fp)

        # FN (False Negatives) are unmatched ground truths
        fn = len([gt for gt in matched_gts if not gt['used']])
        total_confusion_matrix[1, 0] += fn

        # TN (True Negatives) is not calculated for now, as it requires background information
        tn = 0  # Optional: calculate TN using a background category

        # Store the confusion matrix for each category
        confusion_matrices[category_id] = np.array([[tp, fp], [fn, tn]])

        # Compute Precision and Recall
        gt_count = len(category_gts)
        precisions = [t / (t + f) if (t + f) > 0 else 0 for t, f in zip(tps, fps)]
        recalls = [t / gt_count if gt_count > 0 else 0 for t in tps]

        print(f"Precisions for Category {category_id}: {precisions}")
        print(f"Recalls for Category {category_id}: {recalls}")

        # Calculate AP
        ap = compute_ap(precisions, recalls)
        aps.append(ap)
        print(f"AP for Category {category_id}: {ap:.4f}")

        # Store all Precisions and Recalls for all categories
        all_precisions.extend(precisions)
        all_recalls.extend(recalls)

    # Calculate mAP, average Precision, and average Recall for the dataset
    map_value = sum(aps) / len(aps) if aps else 0
    avg_precision = sum(all_precisions) / len(all_precisions) if all_precisions else 0
    avg_recall = sum(all_recalls) / len(all_recalls) if all_recalls else 0

    print(f"\nDataset mAP: {map_value:.4f}")
    print(f"Average Precision: {avg_precision:.4f}")
    print(f"Average Recall: {avg_recall:.4f}")

    return map_value, avg_precision, avg_recall, total_confusion_matrix, confusion_matrices


# Load Ground Truth data
gt_path = '/content/coco/annotations/annotations/instances_val2017.json'  # Modify to your actual path
if not os.path.exists(gt_path):
    raise FileNotFoundError(f"Ground Truth file {gt_path} not found!")

with open(gt_path, 'r') as f:
    gt_data = json.load(f)
    if 'annotations' not in gt_data:
        raise KeyError("Ground Truth JSON file is missing the 'annotations' field!")
    ground_truths = gt_data['annotations']

# Extract category IDs
categories = list({gt.get('category_id') for gt in ground_truths if 'category_id' in gt})

# Load prediction data
predictions_folder = '/content/coco/predictions'  # Modify to your actual path
prediction_files = [f for f in os.listdir(predictions_folder) if f.endswith('.json')]

if not prediction_files:
    raise FileNotFoundError("No JSON files found in the predictions folder!")

all_predictions = []
for prediction_file in prediction_files:
    prediction_path = os.path.join(predictions_folder, prediction_file)
    with open(prediction_path, 'r') as f:
        predictions = json.load(f)
        if not all(key in predictions[0] for key in ('bbox', 'category_id', 'score')):
            raise KeyError(f"Prediction file {prediction_file} is missing required fields!")
        all_predictions.extend(predictions)

# Compute overall dataset mAP, average Precision, Recall, and generate confusion matrices
map_value, avg_precision, avg_recall, total_confusion_matrix, confusion_matrices = compute_dataset_metrics(
    all_predictions, ground_truths, categories, iou_threshold=0.5
)

# Output final results and confusion matrices
print("\nFinal Results:")
print(f"Overall mAP: {map_value:.4f}")
print(f"Average Precision: {avg_precision:.4f}")
print(f"Average Recall: {avg_recall:.4f}")

print("\nTotal Confusion Matrix:")
print(f"[[TP: {total_confusion_matrix[0, 0]}, FP: {total_confusion_matrix[0, 1]}]")
print(f" [FN: {total_confusion_matrix[1, 0]}, TN: {total_confusion_matrix[1, 1]}]]")

print("\nConfusion Matrices per Category:")
for category_id, matrix in confusion_matrices.items():
    print(f"Confusion Matrix for Category {category_id}:\n{matrix}")

"""# The following is our own dataset"""

import cv2
import numpy as np
import os
# Set the image path
image_path = "/content/coco/own_dataset/387801878132186427.jpg"

# Load the image
image = cv2.imread(image_path)

# Get the original height and width of the image
height, width = image.shape[:2]

# Calculate the scaling factor
scale = 416 / max(height, width)
new_height = int(height * scale)
new_width = int(width * scale)

# Resize the image
image_resized = cv2.resize(image, (new_width, new_height))

# Calculate padding for the borders
top_pad = (416 - new_height) // 2
bottom_pad = 416 - new_height - top_pad
left_pad = (416 - new_width) // 2
right_pad = 416 - new_width - left_pad

# Add padding to the image with gray color [128, 128, 128]
image_padded = cv2.copyMakeBorder(image_resized, top_pad, bottom_pad, left_pad, right_pad, cv2.BORDER_CONSTANT, value=[128, 128, 128])

# Save the preprocessed image
preprocessed_image_path = "/content/coco/processed_own_dataset/387801878132186427.jpg"  # Add the file extension
cv2.imwrite(preprocessed_image_path, image_padded)

!./darknet detect cfg/yolov3.cfg yolov3.weights /content/coco/processed_own_dataset/387801878132186427.jpg -out_output_path /content/coco/output_own_image

import cv2

# read image
img = cv2.imread('predictions.jpg')

# Save image to specified folder
cv2.imwrite('/content/coco/output_own_image/processed_image_1.jpg', img)

# show image
from google.colab.patches import cv2_imshow
cv2_imshow(img)

!./darknet detect cfg/yolov3.cfg yolov3.weights /content/coco/own_dataset/387801878132186427.jpg -out /content/coco/output_own_image

import cv2
from google.colab.patches import cv2_imshow

# Read result image
img = cv2.imread('predictions.jpg')

# Save image to specified folder
cv2.imwrite('/content/coco/output_own_image/processed_image_2.jpg', img)

# Display image
cv2_imshow(img)

"""# Convert the json file generated by labelme into coco format"""

import json
import os
from PIL import Image

# Input and output paths
labelme_json_folder = '/content/coco/own_dataset_gt'
output_coco_json_path = '/content/coco/own_dataset_gt_coco'
image_folder = '/content/coco/own_dataset'

# COCO format data structure
coco_data = {
    "images": [],
    "annotations": [],
    "categories": []
}

# Get all JSON files in the folder
json_files = [f for f in os.listdir(labelme_json_folder) if f.endswith('.json')]

# Used to record categories
categories_set = {}
annotations = []
annotation_id = 0

for json_file in json_files:
    labelme_json_path = os.path.join(labelme_json_folder, json_file)

    # Read the Labelme JSON file
    with open(labelme_json_path, 'r', encoding='utf-8') as f:
        labelme_data = json.load(f)

    # Extract the image file name
    original_image_path = labelme_data['imagePath']
    image_file_name = os.path.basename(original_image_path.replace("\\", "/"))  # Extract the file name
    image_full_path = os.path.join(image_folder, image_file_name)

    # Check if the image exists
    if not os.path.exists(image_full_path):
        print(f"Warning: Image file {image_full_path} does not exist, skipping this file!")
        continue

    # Get the image width and height
    with Image.open(image_full_path) as img:
        width, height = img.size

    # Use the file name without extension as image_id
    image_id = os.path.splitext(image_file_name)[0]

    # Add image information to COCO data
    coco_data["images"].append({
        "id": image_id,  # Use the file name as the ID
        "file_name": image_file_name,
        "width": width,
        "height": height
    })

    # Get annotation information
    for shape in labelme_data['shapes']:
        points = shape['points']
        label = shape['label']
        group_id = shape['group_id']  # Use group_id as category_id

        # If the category is not recorded, add it to the category list
        if group_id not in categories_set:
            categories_set[group_id] = {
                "id": group_id,
                "name": label,
                "supercategory": label
            }
            coco_data["categories"].append(categories_set[group_id])

        # Convert to COCO format annotation
        x_min = min([point[0] for point in points])
        y_min = min([point[1] for point in points])
        x_max = max([point[0] for point in points])
        y_max = max([point[1] for point in points])

        annotation_id += 1
        annotations.append({
            "id": annotation_id,
            "image_id": image_id,  # Use the file name as the ID
            "category_id": group_id,  # Use group_id as category_id
            "segmentation": [],
            "area": (x_max - x_min) * (y_max - y_min),
            "bbox": [x_min, y_min, x_max - x_min, y_max - y_min],
            "iscrowd": 0
        })

# Add annotations to COCO data
coco_data["annotations"] = annotations

# Save as COCO format JSON file
with open(output_coco_json_path, 'w', encoding='utf-8') as f:
    json.dump(coco_data, f, ensure_ascii=False, indent=4)

print(f"Conversion complete, COCO format data has been saved to {output_coco_json_path}")

import random
import os

# Set the validation set image path
val_images_dir = '/content/coco/own_dataset'

# Gets all image paths of the validation set
val_images = [os.path.join(val_images_dir, img) for img in os.listdir(val_images_dir) if img.endswith('.jpg')]

# 10 images were randomly selected
sample_images = random.sample(val_images, 10)

# Save the selected 10 image paths to a text file
sample_file = '/content/coco/annotations/own_dataset.txt'
with open(sample_file, 'w') as f:
    for img in sample_images:
        f.write(img + '\n')

print(f"The {len(sample_images)} image has been selected and the path has been saved to {sample_file}")

import os
import json
import re

# Create output directory
output_dir = '/content/coco/predictions_own_dataset'
os.makedirs(output_dir, exist_ok=True)

# Delete previous prediction files
for filename in os.listdir(output_dir):
    file_path = os.path.join(output_dir, filename)
    if os.path.isfile(file_path):
        os.remove(file_path)

# Read image paths
with open('/content/coco/annotations/own_dataset.txt', 'r') as f:
    image_paths = [path.strip() for path in f.readlines()]

# Class mapping
class_name_to_coco_id = {k.lower(): v for k, v in {
    'person': 1,
    'bicycle': 2,
    'car': 3,
    'motorbike': 4,
    'airplane': 5,
    'bus': 6,
    'train': 7,
    'truck': 8,
    'boat': 9,
    'traffic light': 10,
    'fire hydrant': 11,
    'stop sign': 12,
    'parking meter': 13,
    'bench': 14,
    'bird': 15,
    'cat': 16,
    'dog': 17,
    'horse': 18,
    'sheep': 19,
    'cow': 20,
    'elephant': 21,
    'bear': 22,
    'zebra': 23,
    'giraffe': 24,
    'bag': 25,
    'backpack': 26,
    'umbrella': 27,
    'handbag': 28,
    'tie': 29,
    'suitcase': 30,
    'frisbee': 31,
    'skis': 32,
    'snowboard': 33,
    'sports ball': 34,
    'kite': 35,
    'baseball bat': 36,
    'baseball glove': 37,
    'skateboard': 38,
    'surfboard': 39,
    'tennis racket': 40,
    'bottle': 41,
    'wine glass': 42,
    'cup': 43,
    'fork': 44,
    'knife': 45,
    'spoon': 46,
    'bowl': 47,
    'banana': 48,
    'apple': 49,
    'sandwich': 50,
    'orange': 51,
    'broccoli': 52,
    'carrot': 53,
    'hot dog': 54,
    'pizza': 55,
    'donut': 56,
    'cake': 57,
    'chair': 58,
    'couch': 59,
    'potted plant': 60,
    'bed': 61,
    'dining table': 62,
    'toilet': 63,
    'tv': 64,
    'laptop': 65,
    'mouse': 66,
    'remote': 67,
    'keyboard': 68,
    'cell phone': 69,
    'microwave': 70,
    'oven': 71,
    'toaster': 72,
    'sink': 73,
    'refrigerator': 74,
    'book': 75,
    'clock': 76,
    'vase': 77,
    'scissors': 78,
    'teddy bear': 79,
    'hair drier': 80
}.items()}

# YOLOv3 detection and result saving
for img_path in image_paths:
    img_name = os.path.splitext(os.path.basename(img_path))[0]
    yolo_output_file = f'{output_dir}/{img_name}_predictions.txt'

    # Call YOLOv3 detection
    os.system(f'./darknet detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights {img_path} -ext_output > {yolo_output_file}')

    # Parse detection results
    predictions = []
    with open(yolo_output_file, 'r') as f:
        lines = f.readlines()
        for line in lines:
            match = re.match(r'([\w\s]+):\s+(\d+)%\s+\(left_x:\s+(\d+)\s+top_y:\s+(\d+)\s+width:\s+(\d+)\s+height:\s+(\d+)\)', line.strip())
            if match:
                class_name = match.group(1).strip().lower()
                confidence = float(match.group(2)) / 100
                left_x, top_y, width, height = map(int, match.groups()[2:])

                # Class mapping
                category_id = class_name_to_coco_id.get(class_name)
                if category_id:
                    predictions.append({
                        'image_id': img_name,
                        'category_id': category_id,
                        'bbox': [left_x, top_y, width, height],
                        'score': confidence
                    })

    # Save to JSON
    output_json_file = f'{output_dir}/{img_name}_predictions.json'
    with open(output_json_file, 'w') as out_file:
        json.dump(predictions, out_file, indent=4)

    print(f'Processed {img_name}, predictions saved to {output_json_file}')

"""Own Dataset"""

import json
import os
import numpy as np

def compute_iou(box1, box2):
    """Calculate IoU (Intersection over Union) of two bounding boxes."""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[0] + box1[2], box2[0] + box2[2])
    y2 = min(box1[1] + box1[3], box2[1] + box2[3])

    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = box1[2] * box1[3]
    area2 = box2[2] * box2[3]
    union = area1 + area2 - intersection

    return intersection / union if union > 0 else 0

def compute_ap(precisions, recalls):
    """Calculate the AP (Average Precision) value."""
    recalls = [0] + recalls + [1]
    precisions = [0] + precisions + [0]

    for i in range(len(precisions) - 1, 0, -1):
        precisions[i - 1] = max(precisions[i - 1], precisions[i])

    indices = [i for i in range(1, len(recalls)) if recalls[i] != recalls[i - 1]]
    ap = sum((recalls[i] - recalls[i - 1]) * precisions[i] for i in indices)
    return ap

def compute_dataset_metrics(predictions, ground_truths, categories, iou_threshold=0.5):
    """Compute overall dataset mAP, average Precision, Recall, and generate a total confusion matrix."""
    all_precisions = []
    all_recalls = []
    aps = []

    # Initialize the total confusion matrix
    total_confusion_matrix = np.zeros((2, 2), dtype=int)  # [TP, FP; FN, TN]
    confusion_matrices = {}

    for category_id in categories:
        print(f"\n### Calculating AP for Category {category_id} ###")

        # Filter predictions and ground truths by category
        category_preds = [pred for pred in predictions if pred.get('category_id') == category_id]
        category_gts = [gt for gt in ground_truths if gt.get('category_id') == category_id]

        # Sort predictions by score in descending order
        category_preds = sorted(category_preds, key=lambda x: x.get('score', 0), reverse=True)

        # Mark whether ground truths are used (to avoid modifying the original list)
        matched_gts = [dict(gt, used=False) for gt in category_gts]

        tp, fp = 0, 0
        tps, fps = [], []

        for pred in category_preds:
            max_iou = 0
            matched_gt = None

            for gt in matched_gts:
                if gt['used']:
                    continue
                iou = compute_iou(pred['bbox'], gt['bbox'])
                if iou > max_iou:
                    max_iou = iou
                    matched_gt = gt

            if max_iou >= iou_threshold:
                tp += 1
                total_confusion_matrix[0, 0] += 1  # TP
                if matched_gt is not None:
                    matched_gt['used'] = True
            else:
                fp += 1
                total_confusion_matrix[0, 1] += 1  # FP

            # Accumulate TP and FP
            tps.append(tp)
            fps.append(fp)

        # FN (False Negatives) are unmatched ground truths
        fn = len([gt for gt in matched_gts if not gt['used']])
        total_confusion_matrix[1, 0] += fn

        # TN (True Negatives) is not calculated for now, as it requires background information
        tn = 0  # Optional: calculate TN using a background category

        # Store the confusion matrix for each category
        confusion_matrices[category_id] = np.array([[tp, fp], [fn, tn]])

        # Compute Precision and Recall
        gt_count = len(category_gts)
        precisions = [t / (t + f) if (t + f) > 0 else 0 for t, f in zip(tps, fps)]
        recalls = [t / gt_count if gt_count > 0 else 0 for t in tps]

        print(f"Precisions for Category {category_id}: {precisions}")
        print(f"Recalls for Category {category_id}: {recalls}")

        # Calculate AP
        ap = compute_ap(precisions, recalls)
        aps.append(ap)
        print(f"AP for Category {category_id}: {ap:.4f}")

        # Store all Precisions and Recalls for all categories
        all_precisions.extend(precisions)
        all_recalls.extend(recalls)

    # Calculate mAP, average Precision, and average Recall for the dataset
    map_value = sum(aps) / len(aps) if aps else 0
    avg_precision = sum(all_precisions) / len(all_precisions) if all_precisions else 0
    avg_recall = sum(all_recalls) / len(all_recalls) if all_recalls else 0

    print(f"\nDataset mAP: {map_value:.4f}")
    print(f"Average Precision: {avg_precision:.4f}")
    print(f"Average Recall: {avg_recall:.4f}")

    return map_value, avg_precision, avg_recall, total_confusion_matrix, confusion_matrices


# Load Ground Truth data
gt_path = '/content/coco/own_dataset_gt_coco'  # Modify to your actual path
if not os.path.exists(gt_path):
    raise FileNotFoundError(f"Ground Truth file {gt_path} not found!")

with open(gt_path, 'r') as f:
    gt_data = json.load(f)
    if 'annotations' not in gt_data:
        raise KeyError("Ground Truth JSON file is missing the 'annotations' field!")
    ground_truths = gt_data['annotations']

# Extract category IDs
categories = list({gt.get('category_id') for gt in ground_truths if 'category_id' in gt})

# Load prediction data
predictions_folder = '/content/coco/predictions_own_dataset/'  # Modify to your actual path
prediction_files = [f for f in os.listdir(predictions_folder) if f.endswith('.json')]

if not prediction_files:
    raise FileNotFoundError("No JSON files found in the predictions folder!")

all_predictions = []
for prediction_file in prediction_files:
    prediction_path = os.path.join(predictions_folder, prediction_file)
    with open(prediction_path, 'r') as f:
        predictions = json.load(f)
        if not all(key in predictions[0] for key in ('bbox', 'category_id', 'score')):
            raise KeyError(f"Prediction file {prediction_file} is missing required fields!")
        all_predictions.extend(predictions)

# Compute overall dataset mAP, average Precision, Recall, and generate confusion matrices
map_value, avg_precision, avg_recall, total_confusion_matrix, confusion_matrices = compute_dataset_metrics(
    all_predictions, ground_truths, categories, iou_threshold=0.5
)

# Output final results and confusion matrices
print("\nFinal Results:")
print(f"Overall mAP: {map_value:.4f}")
print(f"Average Precision: {avg_precision:.4f}")
print(f"Average Recall: {avg_recall:.4f}")

print("\nTotal Confusion Matrix:")
print(f"[[TP: {total_confusion_matrix[0, 0]}, FP: {total_confusion_matrix[0, 1]}]")
print(f" [FN: {total_confusion_matrix[1, 0]}, TN: {total_confusion_matrix[1, 1]}]]")

print("\nConfusion Matrices per Category:")
for category_id, matrix in confusion_matrices.items():
    print(f"Confusion Matrix for Category {category_id}:\n{matrix}")

